{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukgCuMsShg1w"
   },
   "source": [
    "## Custom NER Model\n",
    "\n",
    "Create a custom Named Entity recognition model for filtering the right Named entites from CVs to help Human resouce Team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzpZJJMotEht"
   },
   "source": [
    "# Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nofXVTd0uCK5"
   },
   "source": [
    "Here's an example of how you can use pre-trained word embeddings in Python using the Gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZNTWqP8tgis"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download pre-trained word embeddings\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV9rnNQNvr_j"
   },
   "source": [
    "We are use=ing the gensim.downloader module to download the pre-trained glove-wiki-gigaword-100 embeddings. You can then use the word_vectors object to access word vectors and perform other operations are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0ZbzVavurbh"
   },
   "outputs": [],
   "source": [
    "# Access word vectors for a given word\n",
    "word_vectors.get_vector(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADLWQuZTuvmr"
   },
   "outputs": [],
   "source": [
    "# Perform similarity calculations between words\n",
    "word_vectors.similarity(\"apple\", \"mango\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n68Xfhpauyic"
   },
   "outputs": [],
   "source": [
    "# check simillarity between king and queen\n",
    "word_vectors.similarity(\"king\", \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhLygtnYu83m"
   },
   "outputs": [],
   "source": [
    "# finding the most simillar words\n",
    "most_similar = word_vectors.most_similar(\"man\")\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfgTd3F_vPMd"
   },
   "outputs": [],
   "source": [
    "# Find the word that best completes an analogy\n",
    "result = word_vectors.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa2jEaohvZcX"
   },
   "source": [
    "These are just a few of the many operations you can perform using pre-trained word embeddings and the gensim library. You can also use these embeddings as features for other NLP tasks, such as text classification or sentiment analysis, by feeding the word vectors into a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmc4q7SBj3Tj"
   },
   "source": [
    "# <H1>NER case study </H1> \n",
    "\n",
    "1.A sample CV taken to extract Named entity recognition\n",
    "\n",
    "2.A pickle file consisting of training data on 200 CVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CLCsKnKlP3G"
   },
   "source": [
    "<H1> Introduction </H1> \n",
    "\n",
    "In this notebook we will be learning about how to set custom named entity recognition (NER) in spacy.\n",
    "\n",
    "**What is Named Entity?**\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title.\n",
    "\n",
    "**About spaCy :-**\n",
    "spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion. \n",
    "\n",
    "**NLTK vs spaCy :-**\n",
    "While NLTK provides access to many algorithms to get something done, spaCy provides the best way to do it. It provides the fastest and most accurate syntactic analysis of any NLP library released to date. It also offers access to larger word vectors that are easier to customize. For an app builder mindset that prioritizes getting features done, spaCy would be the better choice. \n",
    "\n",
    "**Some Features of spaCy :-**\n",
    "\n",
    "Tokenization\n",
    "Part-of-speech (POS) Tagging\n",
    "Lemmatization\n",
    "Named Entity Recognition (NER)\n",
    "Similarity\n",
    "Text Classification\n",
    "We will be focusing on NER.\n",
    "\n",
    "**What is NER ?**\n",
    "Named-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "**What is POS Tagging**\n",
    "\n",
    "Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a corpus in correspondence with a particular part of speech, depending on the definition of the word and its context.\n",
    "\n",
    "This can include nouns, verbs, adjectives, adverbs, determiners etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1866IoT2BSKn"
   },
   "outputs": [],
   "source": [
    "# Install 2.3.7 version of spacy for this exercise. \n",
    "\n",
    "!pip install -U spacy==2.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDhuOMvGBcgE"
   },
   "outputs": [],
   "source": [
    "# Importing spacy lib and checking the version of Spacy\n",
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcPFg-qiBwPP"
   },
   "outputs": [],
   "source": [
    "# Printing & Expalining various Named entities in Spacy\n",
    "print(f'PERSON - {spacy.explain(\"PERSON\")}')\n",
    "print(f'GPE    - {spacy.explain(\"GPE\")}')\n",
    "print(f'DATE   - {spacy.explain(\"DATE\")}')\n",
    "print(f'MONEY  - {spacy.explain(\"MONEY\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoH7sEn2B8Rv"
   },
   "outputs": [],
   "source": [
    "# Importing imp libs. \n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KFmiogrDrqv"
   },
   "outputs": [],
   "source": [
    "# Driving the mount in Google Collab. Pl. put data i.e. Resume and Training pickle file on a folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGfDt6rTEJEx"
   },
   "outputs": [],
   "source": [
    "# The os.walk() function generates the file names in a directory tree by walking the tree either top-down or bottom-up.\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/content/drive/MyDrive/NER Custom Model'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bY94JJMEcrH"
   },
   "outputs": [],
   "source": [
    "# since our resume is in pdf format we will use PyMuPDF to extract data from it.\n",
    "# You can also use PyPDF2.\n",
    "\n",
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWwD7RYNEhaO"
   },
   "outputs": [],
   "source": [
    "import sys,fitz\n",
    "fname = '/content/drive/MyDrive/Prac Data/Alice Clark CV.pdf'\n",
    "doc= fitz.open(fname)\n",
    "alice_cv=\"\"\n",
    "for i in range(doc.page_count):\n",
    "  page=doc.load_page(i)\n",
    "  alice_cv +=page.get_text()\n",
    "\n",
    "print(alice_cv)\n",
    "\n",
    "# we have extracted the data from pdf file using PyMuPDF and stored in alice_cv variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As_mqyRu1GVt"
   },
   "outputs": [],
   "source": [
    "# In this code we are just finding POS and representing it in form of json dumps.\n",
    "# This is just for showcasing how Part of speech tagging is done. \n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(alice_cv)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print out the POS tags\n",
    "print(json.dumps(tagged_tokens,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3J506in3NkC"
   },
   "outputs": [],
   "source": [
    "# We are doing NER via NLTK. We can also do NER via Spacy en_core_web_sm lib. Results are not accurate. \n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(alice_cv)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "entities = nltk.ne_chunk(tagged_tokens)\n",
    "\n",
    "# Print out the entities\n",
    "for token in entities:\n",
    "    if hasattr(token, 'label'):\n",
    "        print(token.label(), ' '.join(c[0] for c in token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKTs44Rr4Web"
   },
   "source": [
    "<H1> Observation </H1> \n",
    "POS is giving quite a good result but using the standard NER model maynot give you the accurate results you are looking for. \n",
    "\n",
    "In this example here are just few in accurate entity extraction. \n",
    "\n",
    "- SQL is not a organization but programming language\n",
    "- Stored procedures is not a person\n",
    "- Stream analytics is not a person. \n",
    "- Karnataka is not a person but a state\n",
    "- Skills is not a person. \n",
    "\n",
    "Hence we will custom train our model just looking into corpus of CVs to make extraction of entites accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6Goyr6vFb2B"
   },
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('/content/drive/MyDrive/Prac Data/train_data.pkl','rb'))\n",
    "print(f\"Training data consist of {len(train_data)} manually labelled resume's.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VS1a7Pk5Fkbc"
   },
   "outputs": [],
   "source": [
    "train_data[96]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cZJS4DI5eWZ"
   },
   "source": [
    "**Anatomy of our train data**\n",
    "\n",
    "Our train data is stored as a tuple consisting of 200 resume data, each resume data consist of 2 parts/indexes.\n",
    "\n",
    "First index [0] consist of all details(name, degree, designation, compaines worked at) in resume.\n",
    "Second index [1] consist of a dictionary object having only one key i.e., 'entities' and look carefully at its value.\n",
    "Value of 'entities' key has a list of tuples and in each tuple we have some number and some labelling.\n",
    "\n",
    "For Eg :- (0, 15, 'Name'), here 0 denotes start index and 15 denotes end index of label 'Name', which is 'Ramesh chokkala'. Similarly, we can see that all the other tuple also has some start and end index alongwith their respective label. This is how you can manually create data for training.\n",
    "\n",
    "Note :- label of all training data should be same i.e., if you have specified label as 'Name' for one resume then for all the resume data wherever name is present for that label should be as 'Name' only and not something else.\n",
    "\n",
    "As we have our training data ready, we will now train our spacy model and add custom NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1t1ILMtFpo2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Creating a function to train our model\n",
    "\n",
    "def train_model(train_data):\n",
    "    if 'ner' not in nlp.pipe_names:# Checking if NER is present in pipeline\n",
    "        ner = nlp.create_pipe('ner')# creating NER pipe if not present\n",
    "        nlp.add_pipe(ner, last = True)# adding NER pipe in the end\n",
    "    \n",
    "    for _, annotation in train_data:# Getting 1 resume at a time from our training data of 200 resumes\n",
    "\n",
    "        for ent in annotation['entities']:# Getting each tuple at a time from 'entities' \n",
    "        #key in dictionary at index[1] i.e.,(0, 15, 'Name') and so on\n",
    "            ner.add_label(ent[2])\n",
    "            \n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    # getting all other pipes except NER.\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            index = 0\n",
    "            for text, annotations in train_data:\n",
    "                try:\n",
    "                    nlp.update(\n",
    "                        [text],  # batch of texts\n",
    "                        [annotations],  # batch of annotations\n",
    "                        drop=0.2,  # dropout - make it harder to memorise data\n",
    "                        sgd=optimizer,  # callable to update weights\n",
    "                        losses=losses)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                \n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBJ3GwIqFvLq"
   },
   "outputs": [],
   "source": [
    "# pass train data to function.\n",
    "\n",
    "train_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9Qy_e4Ty3EH"
   },
   "outputs": [],
   "source": [
    "# Saving our trained model to re-use.\n",
    "\n",
    "nlp.to_disk('/content/drive/MyDrive/NER Custom Model/nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYa0WhX1H9gt"
   },
   "outputs": [],
   "source": [
    "# Loading our trained model\n",
    "\n",
    "nlp_model = spacy.load('/content/drive/MyDrive/NER Custom Model/nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYUmWq5qIIgm"
   },
   "outputs": [],
   "source": [
    "# Checking all the custom NER created\n",
    "nlp_model.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HICGsxMgIMhe"
   },
   "outputs": [],
   "source": [
    "doc = nlp_model(\" \".join(alice_cv.split('\\n')))\n",
    "for ent in doc.ents:\n",
    "  print(f'{ent.label_.upper():{20}} - {ent.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsHyf9ce6Y0g"
   },
   "source": [
    "# <H1>Conclusion </H1>\n",
    "\n",
    "Custom NER is giving far better results than inbuilt Model as can be observed from the results printed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39pBOC5zyC0D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
